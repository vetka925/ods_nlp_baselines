{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "disaster_tweets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "pBMSJz6vV1QC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy textvec\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N7cPdTVMMY8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4245b197-96db-4f33-e299-4093993cd8d9"
      },
      "source": [
        "# Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Preprocessing\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "# ML\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from textvec.vectorizers import TforVectorizer\n",
        "\n",
        "# NNs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Utils\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfN8FyP2bez3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BERT Model\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2YBaxUFV1QJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fbf4c9b-fc20-4b73-f2f9-9e2f379b2689"
      },
      "source": [
        "# Define datafolder\n",
        "# If you are using google colab you can put data in /content/drive/My Drive/Colab/Real-or-Not/data/\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    is_in_colab = True\n",
        "    \n",
        "except:\n",
        "    is_in_colab = False\n",
        "\n",
        "if is_in_colab:\n",
        "    drive.mount('/content/drive')\n",
        "    data_folder = r'/content/drive/My Drive/Colab/Real-or-Not/data/'\n",
        "else:\n",
        "    data_folder = r'./data/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr5xZ5L7NcnT",
        "colab_type": "text"
      },
      "source": [
        "# Look at data and task review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21C-CKh7OC4T",
        "colab_type": "text"
      },
      "source": [
        " We are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0. Its binary classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5q8MdnlNh4v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "outputId": "3bc99d4e-f706-4b48-da21-c01791a42cea"
      },
      "source": [
        "data = pd.read_csv(data_folder + '/train.csv')\n",
        "data.head(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>There's an emergency evacuation happening now ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Three people died from the heat wave so far</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>19</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>23</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What's up man?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>24</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I love fruits</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Summer is lovely</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>My car is so fast</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>28</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>this is ridiculous....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>32</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>London is cool ;)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Love skiing</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What a wonderful day!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>36</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>LOOOOOOL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>37</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No way...I can't eat that shit</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>38</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Was in NYC last week!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>39</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Love my girlfriend</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>40</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cooool :)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>41</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Do you like pasta?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id keyword  ...                                               text target\n",
              "0    1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1    4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2    5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3    6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4    7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "5    8     NaN  ...  #RockyFire Update => California Hwy. 20 closed...      1\n",
              "6   10     NaN  ...  #flood #disaster Heavy rain causes flash flood...      1\n",
              "7   13     NaN  ...  I'm on top of the hill and I can see a fire in...      1\n",
              "8   14     NaN  ...  There's an emergency evacuation happening now ...      1\n",
              "9   15     NaN  ...  I'm afraid that the tornado is coming to our a...      1\n",
              "10  16     NaN  ...        Three people died from the heat wave so far      1\n",
              "11  17     NaN  ...  Haha South Tampa is getting flooded hah- WAIT ...      1\n",
              "12  18     NaN  ...  #raining #flooding #Florida #TampaBay #Tampa 1...      1\n",
              "13  19     NaN  ...            #Flood in Bago Myanmar #We arrived Bago      1\n",
              "14  20     NaN  ...  Damage to school bus on 80 in multi car crash ...      1\n",
              "15  23     NaN  ...                                     What's up man?      0\n",
              "16  24     NaN  ...                                      I love fruits      0\n",
              "17  25     NaN  ...                                   Summer is lovely      0\n",
              "18  26     NaN  ...                                  My car is so fast      0\n",
              "19  28     NaN  ...                       What a goooooooaaaaaal!!!!!!      0\n",
              "20  31     NaN  ...                             this is ridiculous....      0\n",
              "21  32     NaN  ...                                  London is cool ;)      0\n",
              "22  33     NaN  ...                                        Love skiing      0\n",
              "23  34     NaN  ...                              What a wonderful day!      0\n",
              "24  36     NaN  ...                                           LOOOOOOL      0\n",
              "25  37     NaN  ...                     No way...I can't eat that shit      0\n",
              "26  38     NaN  ...                              Was in NYC last week!      0\n",
              "27  39     NaN  ...                                 Love my girlfriend      0\n",
              "28  40     NaN  ...                                          Cooool :)      0\n",
              "29  41     NaN  ...                                 Do you like pasta?      0\n",
              "\n",
              "[30 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoxsIQ1WV1QN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "7921c099-3b3f-4202-ccb1-66040723538d"
      },
      "source": [
        "# Look at the class ratio\n",
        "data.target.hist()\n",
        "data.target.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    7613.00000\n",
              "mean        0.42966\n",
              "std         0.49506\n",
              "min         0.00000\n",
              "25%         0.00000\n",
              "50%         0.00000\n",
              "75%         1.00000\n",
              "max         1.00000\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C60y0sFURiT",
        "colab_type": "text"
      },
      "source": [
        "We have a good balanced dataset. But keep in mind that there are situations when the dataset can be unbalanced so we have to do oversampling or downsampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "XNa-eVSIV1QW",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "CprzzdlxV1Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(text):\n",
        "  text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',    # Substitute different urls with \"url\" token.\n",
        "                  'url', text)\n",
        "  text = re.sub('#', '', text)              # Delete hashtag signs.\n",
        "  text = re.sub('\\d+[.,]?\\d*', 'num', text)  # Substitute different numbers with \"num\" token.                                                 \n",
        "  text = re.sub('@\\w+_?\\w*', 'username', text)  # Substitute different usernames with 'username' token.                                                       \n",
        "  return text\n",
        "\n",
        "def lemmatize(text):\n",
        "  preprocessed_tokens = []    # preprocessed_tokens will be lemmatized, stopwords removed and lowercased\n",
        "  nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])   # We will not use NER and syntactic parser.\n",
        "  doc = nlp(text)\n",
        "  for token in doc:\n",
        "    if not token.is_stop:\n",
        "      preprocessed_tokens.append(token.lemma_.lower())\n",
        "  return ' '.join(preprocessed_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPze5wE-ZYMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['text'] = data.text.apply(clean)    # Clean texts.\n",
        "data['lemmatized_text'] = data.text.apply(lemmatize)  # Add new column with lemmatized and preprocessed texts. This task requires a lot of time.\n",
        "data.drop(['keyword', 'location'], axis=1, inplace=True) # We don't need 'keyword', 'location' columns."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uenp_8EatYT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "69eaa07f-9290-4ef5-d556-b89a1baaa629"
      },
      "source": [
        "data.to_pickle(data_folder + '/cleaned_lemmatized_train.pkl')  # Save preprocessed dataset just in case\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>lemmatized_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
              "      <td>1</td>\n",
              "      <td>deed reason earthquake allah forgive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>forest fire near la ronge sask . canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>resident ask ' shelter place ' notify officer ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>num people receive wildfires evacuation orders...</td>\n",
              "      <td>1</td>\n",
              "      <td>num people receive wildfire evacuation order c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
              "      <td>1</td>\n",
              "      <td>get send photo ruby alaska smoke wildfire pour...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7608</th>\n",
              "      <td>10869</td>\n",
              "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
              "      <td>1</td>\n",
              "      <td>giant crane hold bridge collapse nearby home url</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7609</th>\n",
              "      <td>10870</td>\n",
              "      <td>username username The out of control wild fire...</td>\n",
              "      <td>1</td>\n",
              "      <td>username username control wild fire california...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7610</th>\n",
              "      <td>10871</td>\n",
              "      <td>Mnum [num:num UTC]?numkm S of Volcano Hawaii. url</td>\n",
              "      <td>1</td>\n",
              "      <td>mnum [ num : num utc]?numkm s volcano hawaii ....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7611</th>\n",
              "      <td>10872</td>\n",
              "      <td>Police investigating after an e-bike collided ...</td>\n",
              "      <td>1</td>\n",
              "      <td>police investigate e - bike collide car little...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7612</th>\n",
              "      <td>10873</td>\n",
              "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
              "      <td>1</td>\n",
              "      <td>late : home raze northern california wildfire ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7613 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ...                                    lemmatized_text\n",
              "0         1  ...               deed reason earthquake allah forgive\n",
              "1         4  ...            forest fire near la ronge sask . canada\n",
              "2         5  ...  resident ask ' shelter place ' notify officer ...\n",
              "3         6  ...  num people receive wildfire evacuation order c...\n",
              "4         7  ...  get send photo ruby alaska smoke wildfire pour...\n",
              "...     ...  ...                                                ...\n",
              "7608  10869  ...   giant crane hold bridge collapse nearby home url\n",
              "7609  10870  ...  username username control wild fire california...\n",
              "7610  10871  ...  mnum [ num : num utc]?numkm s volcano hawaii ....\n",
              "7611  10872  ...  police investigate e - bike collide car little...\n",
              "7612  10873  ...  late : home raze northern california wildfire ...\n",
              "\n",
              "[7613 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeCIkIA96gK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_pickle(data_folder+'/cleaned_lemmatized_train.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGiVXA-krkrM",
        "colab_type": "text"
      },
      "source": [
        "## BOW features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaSP7SwJV1Qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data on train and test.\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, data.target, train_size = 0.8, random_state=42)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "V9mYqV6KV1RD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will use TF-IDF and TFOR as features describing texts.\n",
        "# We will try word level features using lemmas and char level features. \n",
        "\n",
        "word_vectorizer = TfidfVectorizer(    # TF-IDF for word level features.\n",
        "    analyzer='word',                  # Word level tokenization.\n",
        "    ngram_range=(1, 1),               # We don't use ngrams. But you can try.\n",
        "    max_features=10000,      # Number of max features (it will be shape of our training matrix: shape=(num_documents,max_features))\n",
        "    max_df=0.7,                        # We will not use tokens that are found in more than 70 percent of documents\n",
        "    min_df=1)                          # We will not use tokens that are found only in 1 document\n",
        "\n",
        "char_vectorizer = TfidfVectorizer(    # TF-IDF for char level features.\n",
        "    analyzer='char',                  # Char level tokenization.                            \n",
        "    ngram_range=(1, 4),               # We use [1..4]grams. \n",
        "    max_features=30000,                                   \n",
        "    max_df=0.7,                                            \n",
        "    min_df=1\n",
        "    )\n",
        "\n",
        "# Get char ngrams features for the train set and the validation set.\n",
        "word_vectorizer.fit(data.lemmatized_text)\n",
        "\n",
        "train_w_features = word_vectorizer.fit_transform(X_train.lemmatized_text)\n",
        "val_w_features = word_vectorizer.transform(X_val.lemmatized_text)\n",
        "\n",
        "# Get lemma features for the train set and the validation set.\n",
        "char_vectorizer.fit(data.text)\n",
        "\n",
        "train_c_features = char_vectorizer.transform(X_train.text)\n",
        "val_c_features = char_vectorizer.transform(X_val.text)\n",
        "\n",
        "# We can use information about classes in features with help of supervised vectorizer. \n",
        "# For example, TFOR (for binary classification) from textvec lib. Let's fit TFOR on char ngrams.\n",
        "tfor = TforVectorizer()\n",
        "\n",
        "tfor_train = tfor.fit_transform(char_vectorizer.transform(X_train.text), y_train)\n",
        "tfor_val = tfor.transform(char_vectorizer.transform(X_val.text))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zcafxVPuV1RF",
        "colab_type": "text"
      },
      "source": [
        "# Classic models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "fVigEZaYV1RG",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8INxpWjgp94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "2bbc3032-b1a8-454b-a3d5-82a49b84b666"
      },
      "source": [
        "# Logreg on tf-idf char ngrams from cleaned texts\n",
        "\n",
        "# You should try different values of C [0.01, 0.1, 1, 10], solver ['liblinear', 'sag'], penalty ['l1', 'l2']\n",
        "\n",
        "logreg = LogisticRegression(C=1, solver='sag')             \n",
        "logreg.fit(train_c_features, y_train)\n",
        "preds = logreg.predict(val_c_features)\n",
        "print(classification_report(y_val, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.90      0.84       874\n",
            "           1       0.83      0.68      0.75       649\n",
            "\n",
            "    accuracy                           0.81      1523\n",
            "   macro avg       0.81      0.79      0.80      1523\n",
            "weighted avg       0.81      0.81      0.80      1523\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf0IdFszhGym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "c5781ba8-35ac-4d56-ab1e-49f2ae42e1ed"
      },
      "source": [
        "# Logreg on tf-idf lowercased lemmas from cleaned, lemmatized, stop words filtered texts.\n",
        "\n",
        "# You should try different values of C [0.01, 0.1, 1, 10], solver ['liblinear', 'sag'], penalty ['l1', 'l2']\n",
        "\n",
        "logreg = LogisticRegression(C=1, solver='sag')               \n",
        "logreg.fit(train_w_features, y_train)\n",
        "preds = logreg.predict(val_w_features)\n",
        "print(classification_report(y_val, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.88      0.83       874\n",
            "           1       0.80      0.67      0.73       649\n",
            "\n",
            "    accuracy                           0.79      1523\n",
            "   macro avg       0.79      0.77      0.78      1523\n",
            "weighted avg       0.79      0.79      0.79      1523\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiXsrOObqb3G",
        "colab_type": "text"
      },
      "source": [
        "**!!! It is possible if lemmatization doesn't work because stop words or special forms of words can be important features.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "covi2J2up5KR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f8d24ae4-0ebf-48b4-d6a4-4fda34aee511"
      },
      "source": [
        "# Logreg on tfor char ngrams from cleaned texts\n",
        "\n",
        "# You should try different values of C [0.01, 0.1, 1, 10], solver ['liblinear', 'sag'], penalty ['l1', 'l2']\n",
        "\n",
        "logreg = LogisticRegression(C=1, solver='sag')            \n",
        "logreg.fit(tfor_train, y_train)\n",
        "preds = logreg.predict(tfor_val)\n",
        "print(classification_report(y_val, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       874\n",
            "           1       0.81      0.71      0.76       649\n",
            "\n",
            "    accuracy                           0.81      1523\n",
            "   macro avg       0.81      0.79      0.80      1523\n",
            "weighted avg       0.81      0.81      0.80      1523\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Tc41-vlP6R",
        "colab_type": "text"
      },
      "source": [
        "TFOR is a little better. But TFOR must give better balance between precision and recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "7p5aufh8V1Re",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEESYlWWZL47",
        "colab_type": "text"
      },
      "source": [
        "## Define functions for learning NNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PrmMt9irV1Rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resolve what Tensors we will use - GPU or CPU. \n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "if is_cuda:\n",
        "    device = torch.device('cuda:0')\n",
        "    from torch.cuda import FloatTensor, LongTensor            # Import GPU Tensors.\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    from torch import FloatTensor, LongTensor                 # Import CPU Tensors."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "8b8PeuBQV1Rv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function for training NN models.\n",
        "\n",
        "def fit(model, loss_function, train_data=None, val_data=None, optimizer=None,\n",
        "        epoch_count=1, batch_size=1, scheduler=None, alpha=1, type_nn=None):\n",
        "  \n",
        "    train_history = []\n",
        "    val_history = []\n",
        "\n",
        "    for epoch in range(epoch_count):\n",
        "            name_prefix = '[{} / {}] '.format(epoch + 1, epoch_count)\n",
        "            epoch_train_score = 0\n",
        "            epoch_val_score = 0\n",
        "            \n",
        "            if train_data:\n",
        "                epoch_train_score = do_epoch(model, loss_function, train_data, batch_size, \n",
        "                                              optimizer, name_prefix + 'Train:', alpha=alpha, type_nn=type_nn,\n",
        "                                             scheduler=scheduler)\n",
        "                train_history.append(epoch_train_score)\n",
        "\n",
        "            if val_data:\n",
        "                name = '  Val:'\n",
        "                if not train_data:\n",
        "                    name = ' Test:'\n",
        "                epoch_val_score = do_epoch(model, loss_function, val_data, batch_size, \n",
        "                                             optimizer=None, name=name_prefix + name, alpha=alpha, type_nn=type_nn,\n",
        "                                           scheduler=scheduler)\n",
        "                \n",
        "                val_history.append(epoch_val_score)\n",
        "\n",
        "    return train_history, val_history\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "hmaj7gvAV1Rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is a function for generating one epoch for each NN model (BERT, FNN, CNN).\n",
        "\n",
        "def do_epoch(model, loss_function, data, batch_size, optimizer=None, name=None, alpha=1, type_nn=None, scheduler=None):\n",
        "    \"\"\"\n",
        "       One epoch generation\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    epoch_loss = 0\n",
        "   \n",
        "    batch_count = len(data)\n",
        "   \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batch_count) as progress_bar:               \n",
        "            for ind, batch in enumerate(data):\n",
        "                if type_nn == 'BERT':\n",
        "                  X_batch, X_mask, y_batch =  batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "                  loss, prediction = model(X_batch, token_type_ids=None, attention_mask=X_mask, labels=y_batch)\n",
        "                elif type_nn == 'CNN':\n",
        "                  X_batch, y_batch = batch\n",
        "                  prediction = model(X_batch)\n",
        "                  loss = loss_f(prediction, y_batch)\n",
        "                else:\n",
        "                  X_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "                  prediction = model(X_batch)\n",
        "                  loss = loss_function(prediction, y_batch)\n",
        "\n",
        "                  for param in model.children():\n",
        "                    if type(param) == nn.Linear:\n",
        "                        loss += alpha * torch.abs(param.weight).sum()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                true_indices = torch.argmax(prediction, dim=1)\n",
        "                correct_samples = torch.sum(true_indices == y_batch).cpu().numpy()\n",
        "                accuracy += correct_samples / y_batch.shape[0]\n",
        "\n",
        "                if is_train:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    if scheduler: scheduler.step(accuracy)\n",
        "              \n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('Epoch {} - accuracy: {:.2f}, loss {:.2f}'.format(\n",
        "                    name, (accuracy / (ind+1)), epoch_loss / (ind+1))\n",
        "                )\n",
        "            \n",
        "            accuracy /= (ind + 1)\n",
        "            epoch_loss /= (ind + 1) \n",
        "            progress_bar.set_description(f'Epoch {name} - accuracy: {accuracy:.2f}, loss: {epoch_loss:.2f}')\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWXCaG_FavJw",
        "colab_type": "text"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arS06iCpa0eX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing data\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4qhTfS0Nx0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data_for_bert(texts):\n",
        "  MAX_LEN = 0\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  for tweet in texts:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_tweet = bert_tokenizer.encode(\n",
        "                        tweet,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        " \n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_tweet)\n",
        " \n",
        "    if len(encoded_tweet) > MAX_LEN:\n",
        "      MAX_LEN = len(encoded_tweet)\n",
        " \n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                        value=0, truncating=\"post\", padding=\"post\")\n",
        "  \n",
        "  # Make attention masks token -> 1, [PAD] -> 0\n",
        "  for tweet in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in tweet]\n",
        "    attention_masks.append(att_mask)\n",
        "    \n",
        "  return input_ids, attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21twlXWlT6F7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare data for BERT\n",
        "input_ids, attention_masks = prepare_data_for_bert(data.text)\n",
        "labels = data.target.values\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.25)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.25)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmJOeMW_dbao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(torch.tensor(train_inputs), torch.tensor(train_masks), torch.tensor(train_labels))\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(torch.tensor(validation_inputs), torch.tensor(validation_masks), torch.tensor(validation_labels))\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ua76ZZAdvFK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a155fbfd-a181-4301-b122-6cba86ca7f59"
      },
      "source": [
        "# Load model\n",
        "\n",
        "bert = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "bert.cuda()   #To GPU"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nng3mn4hhEyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "# Number of training epochs (between 2 and 4 recommended)\n",
        "epochs = 3\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# AdamW is a class from the huggingface library. Read docs for get an idea of parameters.\n",
        "optimizer = AdamW(bert.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5, you can try [1e-5 .. 5e-5]\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-2-mBvvnpFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "5fa47355-5ec5-47f4-89f7-848b865374b7"
      },
      "source": [
        "train_history_bert, val_history_bert = fit(bert, loss_function=None, train_data=train_dataloader, val_data=validation_dataloader, optimizer=optimizer, epoch_count=epochs, batch_size=batch_size, type_nn='BERT', scheduler=scheduler, alpha=1, )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/357 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch [1 / 3] Train: - accuracy: 0.78, loss: 0.47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 357/357 [02:44<00:00,  2.17it/s]\n",
            "Epoch [1 / 3]   Val: - accuracy: 0.83, loss: 0.40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:19<00:00,  6.05it/s]\n",
            "Epoch [2 / 3] Train: - accuracy: 0.86, loss: 0.33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 357/357 [02:44<00:00,  2.17it/s]\n",
            "Epoch [2 / 3]   Val: - accuracy: 0.83, loss: 0.43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:19<00:00,  6.03it/s]\n",
            "Epoch [3 / 3] Train: - accuracy: 0.92, loss: 0.22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 357/357 [02:44<00:00,  2.17it/s]\n",
            "Epoch [3 / 3]   Val: - accuracy: 0.81, loss: 0.56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:19<00:00,  6.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "CXB0QbfsV1Rz",
        "colab_type": "text"
      },
      "source": [
        "## LinearNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUDfkgHRa4pl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare data for NN\n",
        "\n",
        "# Create the DataLoader for our training set. We will use TF-IDF matrix\n",
        "train_data = TensorDataset(torch.FloatTensor(train_c_features.toarray()), torch.tensor(np.array(y_train)))\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.  We will use TF-IDF matrix\n",
        "validation_data = TensorDataset(torch.FloatTensor(val_c_features.toarray()), torch.tensor(np.array(y_val)))\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "OaauF8d3V1R3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit settings\n",
        "batch_size = 32\n",
        "epoch_count = 4\n",
        "\n",
        "# optim settings. You should try different values.\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 0.1\n",
        "alpha = 0.005\n",
        "\n",
        "# model settings. \n",
        "linear1_out = int(val_c_features.shape[1]**0.5)            # You should try different values.\n",
        "output = 2                                                 # Equals to num classes.\n",
        "dropout = 0.3                                              # You should try different values.\n",
        "\n",
        "\n",
        "model = nn.Sequential(nn.Linear(train_c_features.shape[1], linear1_out),\n",
        "                      nn.BatchNorm1d(linear1_out),\n",
        "#                       nn.Dropout(p=dropout, inplace=True),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.Linear(linear1_out, output),\n",
        "                      nn.ReLU(inplace=True)\n",
        "                     ).to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "                        model.parameters(),\n",
        "                        lr=learning_rate, \n",
        "                        weight_decay=weight_decay\n",
        "                    )\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WHOHn2xlGcv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "7fe571a7-59a2-4451-9b4d-fb4657d7efea"
      },
      "source": [
        "train_history, val_history = fit(model, loss_function, train_dataloader, validation_dataloader, optimizer, epoch_count, batch_size, scheduler=None, alpha=alpha)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1 / 4] Train: - accuracy: 0.72, loss: 10.54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 191/191 [00:01<00:00, 108.59it/s]\n",
            "Epoch [1 / 4]   Val: - accuracy: 0.74, loss: 1.51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 180.53it/s]\n",
            "Epoch [2 / 4] Train: - accuracy: 0.76, loss: 1.45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 191/191 [00:01<00:00, 107.35it/s]\n",
            "Epoch [2 / 4]   Val: - accuracy: 0.78, loss: 1.40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 184.77it/s]\n",
            "Epoch [3 / 4] Train: - accuracy: 0.76, loss: 1.37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 191/191 [00:01<00:00, 108.81it/s]\n",
            "Epoch [3 / 4]   Val: - accuracy: 0.77, loss: 1.36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 185.09it/s]\n",
            "Epoch [4 / 4] Train: - accuracy: 0.76, loss: 1.31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 191/191 [00:01<00:00, 108.78it/s]\n",
            "Epoch [4 / 4]   Val: - accuracy: 0.77, loss: 1.29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 186.77it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2C3_pui8k0h",
        "colab_type": "text"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS16z2WLJVJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare data for torch dataset.\n",
        "\n",
        "train, val = train_test_split(data[['text', 'target']], train_size = 0.8, random_state=42)\n",
        "train.to_csv(data_folder + 'train_cnn.csv')\n",
        "val.to_csv(data_folder + 'val_cnn.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCjHko_OEDyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "489347a1-b7f3-4025-d87a-008e3071c038"
      },
      "source": [
        "# Prepare torch dataset\n",
        "\n",
        "import torchtext\n",
        "\n",
        "MAX_TEXT_LEN = max(data.text.apply(lambda x: len(x)))\n",
        "\n",
        "train, val = train_test_split(data[['text', 'target']], train_size = 0.8, random_state=42)\n",
        "\n",
        "text_field = torchtext.data.Field(lower=True, include_lengths=False, fix_length=1000, batch_first=True)\n",
        "target_field = torchtext.data.Field(sequential=False, is_target=True, use_vocab=False)\n",
        "\n",
        "train_dataset = torchtext.data.TabularDataset(data_folder + 'train_cnn.csv', format='csv', fields={'text': ('text', text_field), 'target': ('target', target_field)})\n",
        "val_dataset = torchtext.data.TabularDataset(data_folder + 'val_cnn.csv', format='csv', fields={'text': ('text', text_field), 'target': ('target', target_field)})\n",
        "\n",
        "text_field.build_vocab(train_dataset, min_freq=2)\n",
        "target_field.build_vocab(train_dataset)\n",
        "vocab = text_field.vocab\n",
        "\n",
        "\n",
        "print('Vocab size: ', len(vocab))\n",
        "print(train_dataset[0].text)\n",
        "print(train_dataset[0].target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  5927\n",
            "['courageous', 'and', 'honest', 'analysis', 'of', 'need', 'to', 'use', 'atomic', 'bomb', 'in', 'num', 'hiroshimanum', 'japanese', 'military', 'refused', 'surrender.', 'url']\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PApWawpYFlp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define architecture of our CNN.\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, n_classes,\n",
        "                 kernel_sizes_cnn, filters_cnn: int, dense_size: int,\n",
        "                 dropout_rate: float = 0.,):\n",
        "        super().__init__()\n",
        "\n",
        "        self._n_classes = n_classes\n",
        "        self._vocab_size = vocab_size\n",
        "        self._embedding_size = embedding_size\n",
        "        self._kernel_sizes_cnn = kernel_sizes_cnn\n",
        "        self._filters_cnn = filters_cnn\n",
        "        self._dense_size = dense_size\n",
        "        self._dropout_rate = dropout_rate\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
        "\n",
        "        self.cnns = []\n",
        "        for i in range(len(kernel_sizes_cnn)):\n",
        "            in_channels = embedding_size\n",
        "\n",
        "            cnn = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, filters_cnn, kernel_sizes_cnn[i]),\n",
        "                nn.BatchNorm1d(filters_cnn),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "            cnn.apply(self.init_weights)\n",
        "\n",
        "            self.add_module(f'cnn_{i}', cnn)\n",
        "            self.cnns.append(cnn)\n",
        "        \n",
        "        # concatenated to hidden to classes\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(filters_cnn * len(kernel_sizes_cnn), dense_size),\n",
        "            nn.BatchNorm1d(dense_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(dense_size, n_classes)\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weights(module):\n",
        "        if type(module) == nn.Linear or type(module) == nn.Conv1d:\n",
        "            nn.init.kaiming_normal_(module.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.embedding(x)\n",
        "        x0 = torch.transpose(x0, 1, 2)\n",
        "\n",
        "        outputs0 = []\n",
        "        outputs1 = []\n",
        "\n",
        "        for i in range(len(self.cnns)):\n",
        "            cnn = getattr(self, f'cnn_{i}')\n",
        "            # apply cnn and global max pooling\n",
        "            pooled, _ = cnn(x0).max(dim=2)\n",
        "            outputs0.append(pooled)\n",
        "\n",
        "        x0 = torch.cat(outputs0, dim=1) if len(outputs0) > 1 else outputs0[0]\n",
        "        return self.projection(x0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoEzXo55LJ9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d80d895b-d4bf-408d-a679-b1eb300561e9"
      },
      "source": [
        "# Prepare data loaders.\n",
        "train_loader, val_loader = torchtext.data.Iterator.splits((train_dataset, val_dataset),\n",
        "                                                           batch_sizes=(64, 64),\n",
        "                                                           sort=False,\n",
        "                                                           device='cuda')\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "print(batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.data.batch.Batch of size 64]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 64x1000 (GPU 0)]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 64 (GPU 0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNrXruLJSlRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Init CNN model and parameters.\n",
        "\n",
        "epochs = 8                        # you can different number of epochs. \n",
        "batch_size = 64                   # you can different number of batch size. \n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 300\n",
        "n_classes = 2\n",
        "kernel_sizes = (1, 2, 3, 5)       # You can try different values.\n",
        "dense_size = 256                  # You can try different values.\n",
        "dropout = 0.5                     # You can try different values.\n",
        "filters_size = 512                # You can try different values.\n",
        "\n",
        "model = CNN(vocab_size, embedding_size, n_classe, kernel_sizes,\n",
        "            filters_size, dense_size, dropout)\n",
        "\n",
        "model.cuda()  # move model to GPU\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "# AdamW is a class from the huggingface library\n",
        "\n",
        "# You can try different values of learning rate, i.e. [1e-2, 1e-3, 1e-4, 5e-3, 5e-4] and others. Check learning process to get the best value.\n",
        "\n",
        "optimizer = AdamW(model.parameters(), \n",
        "                  lr=1e-3)                                          \n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 10,  # You can try different values.\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVMtRxqmEIE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "a404a6e3-0860-4db4-b42f-cc6a65a14b24"
      },
      "source": [
        "train_history, val_history = fit(model, loss_f, train_loader, val_loader, optimizer, epochs, batch_size, scheduler=scheduler, type_nn='CNN',)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/96 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch [1 / 8] Train: - accuracy: 0.63, loss: 0.74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [00:45<00:00,  2.10it/s]\n",
            "Epoch [1 / 8]   Val: - accuracy: 0.72, loss: 0.62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:04<00:00,  5.40it/s]\n",
            "Epoch [2 / 8] Train: - accuracy: 0.73, loss: 0.55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [00:46<00:00,  2.07it/s]\n",
            "Epoch [2 / 8]   Val: - accuracy: 0.76, loss: 0.55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:04<00:00,  5.34it/s]\n",
            "Epoch [3 / 8] Train: - accuracy: 0.77, loss: 0.49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [00:46<00:00,  2.07it/s]\n",
            "Epoch [3 / 8]   Val: - accuracy: 0.77, loss: 0.54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:04<00:00,  5.36it/s]\n",
            "Epoch [4 / 8] Train: - accuracy: 0.80, loss: 0.44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [00:46<00:00,  2.07it/s]\n",
            "Epoch [4 / 8]   Val: - accuracy: 0.78, loss: 0.50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:04<00:00,  5.38it/s]\n",
            "Epoch [5 / 8] Train: - accuracy: 0.83, loss: 0.39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [00:46<00:00,  2.07it/s]\n",
            "Epoch [5 / 8]   Val: - accuracy: 0.78, loss: 0.49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:04<00:00,  5.38it/s]\n",
            "Epoch [6 / 8] Train: - accuracy: 0.84, loss: 0.36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [00:46<00:00,  2.07it/s]\n",
            "Epoch [6 / 8]   Val: - accuracy: 0.78, loss: 0.50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:04<00:00,  5.38it/s]\n",
            "Epoch [7 / 8] Train: - accuracy: 0.87, loss: 0.32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [00:46<00:00,  2.07it/s]\n",
            "Epoch [7 / 8]   Val: - accuracy: 0.77, loss: 0.50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:04<00:00,  5.37it/s]\n",
            "Epoch [8 / 8] Train: - accuracy: 0.88, loss: 0.30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [00:46<00:00,  2.07it/s]\n",
            "Epoch [8 / 8]   Val: - accuracy: 0.77, loss: 0.52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:04<00:00,  5.39it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNlB8uwsbn7Z",
        "colab_type": "text"
      },
      "source": [
        "# Resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id6UyJNPbrfg",
        "colab_type": "text"
      },
      "source": [
        "Our best model is BERT with accuracy score = 0.83. But we have lightweight and fast model with accuracy score = 0.81 its the Logistic regression on TF-IDF char level features. Choosing the best model depends of your task and data that you have be smart and experiment."
      ]
    }
  ]
}